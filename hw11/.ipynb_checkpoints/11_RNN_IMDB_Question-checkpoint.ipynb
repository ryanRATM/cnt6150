{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs to classify sentiment on IMDB data\n",
    "For this exercise, we will train a \"vanilla\" RNN to predict the sentiment on IMDB reviews.  Our data consists of 25000 training sequences and 25000 test sequences.  The outcome is binary (positive/negative) and both outcomes are equally represented in both the training and the test set.\n",
    "\n",
    "Keras provides a convenient interface to load the data and immediately encode the words into integers (based on the most common words).  This will save us a lot of the drudgery that is usually involved when working with raw text.\n",
    "\n",
    "We will walk through the preparation of the data and the building of an RNN model.  Then it will be your turn to build your own models (and prepare the data how you see fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "## Load in the data.  The function automatically tokenizes the text into distinct integers\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 30)\n",
      "x_test shape: (25000, 30)\n"
     ]
    }
   ],
   "source": [
    "# This pads (or truncates) the sequences so that they are of the maximum length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  219,   141,    35,   221,   956,    54,    13,    16,    11,\n",
       "        2714,    61,   322,   423,    12,    38,    76,    59,  1803,\n",
       "          72,     8, 10508,    23,     5,   967,    12,    38,    85,\n",
       "          62,   358,    99])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here's what an example sequence looks like\n",
    "x_train[123,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras layers for (Vanilla) RNNs\n",
    "\n",
    "In this exercise, we will not use pre-trained word vectors.  Rather we will learn an embedding as part of the Neural Network.  This is represented by the Embedding Layer below.\n",
    "\n",
    "### Embedding Layer\n",
    "`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n",
    "\n",
    "- This layer maps each integer into a distinct (dense) word vector of length `output_dim`.\n",
    "- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe)\n",
    "- The `input_dim` should be the size of the vocabulary.\n",
    "- The `input_length` specifies the length of the sequences that the network expects.\n",
    "\n",
    "### SimpleRNN Layer\n",
    "`keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- This is the basic RNN, where the output is also fed back as the \"hidden state\" to the next iteration.\n",
    "- The parameter `units` gives the dimensionality of the output (and therefore the hidden state).  Note that typically there will be another layer after the RNN mapping the (RNN) output to the network output.  So we should think of this value as the desired dimensionality of the hidden state and not necessarily the desired output of the network.\n",
    "- Recall that there are two sets of weights, one for the \"recurrent\" phase and the other for the \"kernel\" phase.  These can be configured separately in terms of their initialization, regularization, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's build a RNN\n",
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 50\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim, \n",
    "                        kernel_initializer=initializers.RandomNormal(stddev=0.001), \n",
    "                        recurrent_initializer=initializers.Identity(gain=1.0), \n",
    "                        activation='relu', \n",
    "                        input_shape=x_train.shape[1:])\n",
    "             )\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          1000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 5)                 280       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,000,286\n",
      "Trainable params: 1,000,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Note that most of the parameters come from the embedding layer\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr=0.0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy', \n",
    "                  optimizer=rmsprop, \n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6709 - accuracy: 0.5457 - val_loss: 0.6415 - val_accuracy: 0.6965\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.6141 - accuracy: 0.7059 - val_loss: 0.6068 - val_accuracy: 0.7328\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.5827 - accuracy: 0.7498 - val_loss: 0.5897 - val_accuracy: 0.7284\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.5587 - accuracy: 0.7705 - val_loss: 0.5693 - val_accuracy: 0.7548\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.5388 - accuracy: 0.7854 - val_loss: 0.5555 - val_accuracy: 0.7614\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.5214 - accuracy: 0.7956 - val_loss: 0.5453 - val_accuracy: 0.7626\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.5069 - accuracy: 0.8051 - val_loss: 0.5363 - val_accuracy: 0.7659\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4942 - accuracy: 0.8110 - val_loss: 0.5280 - val_accuracy: 0.7691\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4828 - accuracy: 0.8180 - val_loss: 0.5227 - val_accuracy: 0.7689\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4719 - accuracy: 0.8216 - val_loss: 0.5149 - val_accuracy: 0.7734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21c4c2c9dd8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(x_train, \n",
    "              y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=10, \n",
    "              validation_data=(x_test, y_test)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5149 - accuracy: 0.7734\n",
      "Test score:  0.5149085521697998\n",
      "Test accuracy:  0.7734000086784363\n"
     ]
    }
   ],
   "source": [
    "score, acc = model_rnn.evaluate(x_test, \n",
    "                                y_test, \n",
    "                                batch_size=batch_size)\n",
    "\n",
    "print('Test score: ', score)\n",
    "print('Test accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "### Your Turn\n",
    "\n",
    "Now do it yourself:\n",
    "- Prepare the data to use sequences of length 80 rather than length 30.  Did it improve the performance?\n",
    "- Try different values of the \"max_features\".  Can you improve the performance?\n",
    "- Try smaller and larger sizes of the RNN hidden dimension.  How does it affect the model performance?  How does it affect the run time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6667 - accuracy: 0.6069 - val_loss: 0.6311 - val_accuracy: 0.6745\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.5940 - accuracy: 0.7171 - val_loss: 0.5779 - val_accuracy: 0.7130\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.5230 - accuracy: 0.7553 - val_loss: 0.5311 - val_accuracy: 0.7350\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4717 - accuracy: 0.7804 - val_loss: 0.5033 - val_accuracy: 0.7548\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4378 - accuracy: 0.7999 - val_loss: 0.4834 - val_accuracy: 0.7664\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.4133 - accuracy: 0.8143 - val_loss: 0.4731 - val_accuracy: 0.7718\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.3950 - accuracy: 0.8230 - val_loss: 0.4605 - val_accuracy: 0.7798\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.3806 - accuracy: 0.8298 - val_loss: 0.4508 - val_accuracy: 0.7852\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.3699 - accuracy: 0.8331 - val_loss: 0.4491 - val_accuracy: 0.7874\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.3614 - accuracy: 0.8394 - val_loss: 0.4488 - val_accuracy: 0.7894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21c58cb2320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the sequence length from 80 to 30\n",
    "max_features_mod1 = 20000\n",
    "maxlen_mod1 = 30\n",
    "\n",
    "(x_train_mod1, y_train_mod1), (x_test_mod1, y_test_mod1) = imdb.load_data(num_words=max_features_mod1)\n",
    "x_train_mod1 = sequence.pad_sequences(x_train_mod1, maxlen=maxlen_mod1)\n",
    "x_test_mod1 = sequence.pad_sequences(x_test_mod1, maxlen=maxlen_mod1)\n",
    "\n",
    "rnn_hidden_dim_mod1 = 5\n",
    "word_embedding_dim_mod1 = 50\n",
    "\n",
    "model_rnn_mod1 = Sequential()\n",
    "model_rnn_mod1.add(Embedding(max_features_mod1, word_embedding_dim_mod1))\n",
    "model_rnn_mod1.add(SimpleRNN(rnn_hidden_dim_mod1, \n",
    "                        kernel_initializer=initializers.RandomNormal(stddev=0.001), \n",
    "                        recurrent_initializer=initializers.Identity(gain=1.0), \n",
    "                        activation='relu', \n",
    "                        input_shape=x_train_mod1.shape[1:])\n",
    "             )\n",
    "model_rnn_mod1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "rmsprop_mod1 = keras.optimizers.RMSprop(lr=0.0001)\n",
    "\n",
    "model_rnn_mod1.compile(loss='binary_crossentropy', \n",
    "                  optimizer=rmsprop_mod1, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_rnn_mod1.fit(x_train_mod1, \n",
    "              y_train_mod1, \n",
    "              batch_size=batch_size, \n",
    "              epochs=10, \n",
    "              validation_data=(x_test_mod1, y_test_mod1)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Only changing the sequence length from 80 to 30 showed a greater accuracy on the testing and validation sets, however, as each iteration progressed that difference narrowed.  At the end the RNN with the sequence length was still slightly above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6548 - accuracy: 0.6271 - val_loss: 0.6037 - val_accuracy: 0.6934\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 0.5526 - accuracy: 0.7331 - val_loss: 0.5363 - val_accuracy: 0.7309\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.4847 - accuracy: 0.7733 - val_loss: 0.5096 - val_accuracy: 0.7485\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.4427 - accuracy: 0.7963 - val_loss: 0.4783 - val_accuracy: 0.7682\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.4134 - accuracy: 0.8158 - val_loss: 0.4663 - val_accuracy: 0.7758\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3919 - accuracy: 0.8263 - val_loss: 0.4545 - val_accuracy: 0.7809\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3762 - accuracy: 0.8356 - val_loss: 0.4547 - val_accuracy: 0.7834\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3640 - accuracy: 0.8404 - val_loss: 0.4444 - val_accuracy: 0.7886\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3548 - accuracy: 0.8456 - val_loss: 0.4453 - val_accuracy: 0.7878\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3478 - accuracy: 0.8487 - val_loss: 0.4452 - val_accuracy: 0.7918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19486080c50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play with max features\n",
    "max_features_mod2 = 40000\n",
    "maxlen_mod2 = 30\n",
    "\n",
    "(x_train_mod2, y_train_mod2), (x_test_mod2, y_test_mod2) = imdb.load_data(num_words=max_features_mod2)\n",
    "x_train_mod2 = sequence.pad_sequences(x_train_mod2, maxlen=maxlen_mod2)\n",
    "x_test_mod2 = sequence.pad_sequences(x_test_mod2, maxlen=maxlen_mod2)\n",
    "\n",
    "rnn_hidden_dim_mod2 = 5\n",
    "word_embedding_dim_mod2 = 50\n",
    "\n",
    "model_rnn_mod2 = Sequential()\n",
    "model_rnn_mod2.add(Embedding(max_features_mod2, word_embedding_dim_mod2))\n",
    "model_rnn_mod2.add(SimpleRNN(rnn_hidden_dim_mod2, \n",
    "                        kernel_initializer=initializers.RandomNormal(stddev=0.001), \n",
    "                        recurrent_initializer=initializers.Identity(gain=1.0), \n",
    "                        activation='relu', \n",
    "                        input_shape=x_train_mod2.shape[1:])\n",
    "             )\n",
    "model_rnn_mod2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "rmsprop_mod2 = keras.optimizers.RMSprop(lr=0.0001)\n",
    "\n",
    "model_rnn_mod2.compile(loss='binary_crossentropy', \n",
    "                  optimizer=rmsprop_mod2, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_rnn_mod2.fit(x_train_mod2, \n",
    "              y_train_mod2, \n",
    "              batch_size=batch_size, \n",
    "              epochs=10, \n",
    "              validation_data=(x_test_mod2, y_test_mod2)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Setting ```max_feature``` to 40,000 resulted in the highest validation set accuracy of ~79%. However lowering the ```max_feature``` all the way down to 2,500 didn't affect the accuracy below 78%.  In fact any value greater than 40,000 for the ```max_feature``` resulted in the accuracy to decrease towards 78%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.6415 - accuracy: 0.6261 - val_loss: 0.5815 - val_accuracy: 0.6882\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.5311 - accuracy: 0.7320 - val_loss: 0.5218 - val_accuracy: 0.7358\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4674 - accuracy: 0.7808 - val_loss: 0.4880 - val_accuracy: 0.7602\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4278 - accuracy: 0.8051 - val_loss: 0.4690 - val_accuracy: 0.7725\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.4014 - accuracy: 0.8181 - val_loss: 0.4646 - val_accuracy: 0.7761\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3831 - accuracy: 0.8277 - val_loss: 0.4557 - val_accuracy: 0.7820\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3697 - accuracy: 0.8364 - val_loss: 0.4628 - val_accuracy: 0.7809\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 16s 21ms/step - loss: 0.3583 - accuracy: 0.8420 - val_loss: 0.4561 - val_accuracy: 0.7853\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3496 - accuracy: 0.8474 - val_loss: 0.4506 - val_accuracy: 0.7884\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 16s 20ms/step - loss: 0.3424 - accuracy: 0.8499 - val_loss: 0.4516 - val_accuracy: 0.7916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x194862b3da0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play with number of hidden dimensions\n",
    "max_features_mod3 = 40000\n",
    "maxlen_mod3 = 30\n",
    "\n",
    "(x_train_mod3, y_train_mod3), (x_test_mod3, y_test_mod3) = imdb.load_data(num_words=max_features_mod3)\n",
    "x_train_mod3 = sequence.pad_sequences(x_train_mod3, maxlen=maxlen_mod3)\n",
    "x_test_mod3 = sequence.pad_sequences(x_test_mod3, maxlen=maxlen_mod3)\n",
    "\n",
    "rnn_hidden_dim_mod3 = 25\n",
    "word_embedding_dim_mod3 = 50\n",
    "\n",
    "model_rnn_mod3 = Sequential()\n",
    "model_rnn_mod3.add(Embedding(max_features_mod3, word_embedding_dim_mod3))\n",
    "model_rnn_mod3.add(SimpleRNN(rnn_hidden_dim_mod3, \n",
    "                        kernel_initializer=initializers.RandomNormal(stddev=0.001), \n",
    "                        recurrent_initializer=initializers.Identity(gain=1.0), \n",
    "                        activation='relu', \n",
    "                        input_shape=x_train_mod3.shape[1:])\n",
    "             )\n",
    "model_rnn_mod3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "rmsprop_mod3 = keras.optimizers.RMSprop(lr=0.0001)\n",
    "\n",
    "model_rnn_mod3.compile(loss='binary_crossentropy', \n",
    "                  optimizer=rmsprop_mod3, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_rnn_mod3.fit(x_train_mod3, \n",
    "              y_train_mod3, \n",
    "              batch_size=batch_size, \n",
    "              epochs=10, \n",
    "              validation_data=(x_test_mod3, y_test_mod3)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Comparing the model above with the previous RNN model by increasing the number of hidden dimensions increases the model performance.  Atleast by increasing the number of hidden dimensions from 5 to 25 doesn't affect the training time, unlike when we increase the ```max_features``` hyper-parameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
