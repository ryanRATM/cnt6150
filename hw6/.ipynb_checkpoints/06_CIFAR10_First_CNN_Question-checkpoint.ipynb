{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN to classify images in the CIFAR-10 Dataset\n",
    "\n",
    "We will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The 10 classes are:\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> airplane\n",
    "<li>  automobile\n",
    "<li> bird\n",
    "<li>  cat\n",
    "<li> deer\n",
    "<li> dog\n",
    "<li>  frog\n",
    "<li>  horse\n",
    "<li>  ship\n",
    "<li>  truck\n",
    "</ol>\n",
    "\n",
    "For details about CIFAR-10 see:\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "For a compilation of published performance results on CIFAR 10, see:\n",
    "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
    "\n",
    "---\n",
    "\n",
    "### Building Convolutional Neural Nets\n",
    "\n",
    "In this exercise we will build and train our first convolutional neural networks.  In the first part, we walk through the different layers and how they are configured.  In the second part, you will build your own model, train it, and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 32s 0us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000  train samples\n",
      "10000  test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], ' train samples')\n",
    "print(x_test.shape[0], ' test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Each image is a 32 x 32 x 3 numpy array\n",
    "x_train[444].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21cae2a8a20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's look at one of the images\n",
    "print(y_train[444])\n",
    "plt.imshow(x_train[444])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now instead of classes described by an integer between 0-9 we have a vector with a 1 in the (Pythonic) 9th position\n",
    "y_train[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, let's make everything float and scale\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test  /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Layers for CNNs\n",
    "- Previously we built Neural Networks using primarily the Dense, Activation and Dropout Layers.\n",
    "\n",
    "- Here we will describe how to use some of the CNN-specific layers provided by Keras\n",
    "\n",
    "### Conv2D\n",
    "\n",
    "```python\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "```\n",
    "\n",
    "A few parameters explained:\n",
    "- `filters`: the number of filter used per location.  In other words, the depth of the output.\n",
    "- `kernel_size`: an (x,y) tuple giving the height and width of the kernel to be used\n",
    "- `strides`: and (x,y) tuple giving the stride in each dimension.  Default is `(1,1)`\n",
    "- `input_shape`: required only for the first layer\n",
    "\n",
    "Note, the size of the output will be determined by the kernel_size, strides\n",
    "\n",
    "### MaxPooling2D\n",
    "`keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "- `pool_size`: the (x,y) size of the grid to be pooled.\n",
    "- `strides`: Assumed to be the `pool_size` unless otherwise specified\n",
    "\n",
    "### Flatten\n",
    "Turns its input into a one-dimensional vector (per instance).  Usually used when transitioning between convolutional layers and fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "## First CNN\n",
    "Below we will build our first CNN.  For demonstration purposes (so that it will train quickly) it is not very deep and has relatively few parameters.  We use strides of 2 in the first two convolutional layers which quickly reduces the dimensions of the output.  After a MaxPooling layer, we flatten, and then have a single fully connected layer before our final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 32)        2432      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               147968    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "model_1 = Sequential([\n",
    "    Conv2D(32, (5, 5), strides=(2,2), padding='same', input_shape=x_train.shape[1:], activation='relu'),\n",
    "    \n",
    "    Conv2D(32, (5,5), strides=(2,2), activation='relu'),\n",
    "    \n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    \n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 181K parameters, even though this is a \"small\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "batch_size = 32\n",
    "\n",
    "opt = keras.optimizers.RMSprop(lr=.0005, decay=1e-6)\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "               optimizer=opt,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "50000/50000 [==============================] - 23s 470us/sample - loss: 1.7208 - acc: 0.3721 - val_loss: 1.4095 - val_acc: 0.4981\n",
      "Epoch 2/15\n",
      "50000/50000 [==============================] - 23s 468us/sample - loss: 1.4408 - acc: 0.4816 - val_loss: 1.3060 - val_acc: 0.5302\n",
      "Epoch 3/15\n",
      "50000/50000 [==============================] - 24s 483us/sample - loss: 1.3325 - acc: 0.5243 - val_loss: 1.2798 - val_acc: 0.5579\n",
      "Epoch 4/15\n",
      "50000/50000 [==============================] - 23s 460us/sample - loss: 1.2617 - acc: 0.5521 - val_loss: 1.4147 - val_acc: 0.5152\n",
      "Epoch 5/15\n",
      "50000/50000 [==============================] - 20s 409us/sample - loss: 1.2127 - acc: 0.5733 - val_loss: 1.1310 - val_acc: 0.5986\n",
      "Epoch 6/15\n",
      "50000/50000 [==============================] - 21s 415us/sample - loss: 1.1853 - acc: 0.5847 - val_loss: 1.1204 - val_acc: 0.6011\n",
      "Epoch 7/15\n",
      "50000/50000 [==============================] - 21s 414us/sample - loss: 1.1589 - acc: 0.5925 - val_loss: 1.1006 - val_acc: 0.6076\n",
      "Epoch 8/15\n",
      "50000/50000 [==============================] - 21s 423us/sample - loss: 1.1365 - acc: 0.6035 - val_loss: 1.1917 - val_acc: 0.5927\n",
      "Epoch 9/15\n",
      "50000/50000 [==============================] - 21s 424us/sample - loss: 1.1195 - acc: 0.6078 - val_loss: 1.1233 - val_acc: 0.6101\n",
      "Epoch 10/15\n",
      "50000/50000 [==============================] - 21s 411us/sample - loss: 1.1062 - acc: 0.6145 - val_loss: 1.0567 - val_acc: 0.6277\n",
      "Epoch 11/15\n",
      "50000/50000 [==============================] - 23s 464us/sample - loss: 1.0925 - acc: 0.6183 - val_loss: 1.0391 - val_acc: 0.6332\n",
      "Epoch 12/15\n",
      "50000/50000 [==============================] - 21s 421us/sample - loss: 1.0883 - acc: 0.6212 - val_loss: 1.0542 - val_acc: 0.6385\n",
      "Epoch 13/15\n",
      "50000/50000 [==============================] - 23s 465us/sample - loss: 1.0771 - acc: 0.6264 - val_loss: 1.1271 - val_acc: 0.6285\n",
      "Epoch 14/15\n",
      "50000/50000 [==============================] - 21s 425us/sample - loss: 1.0708 - acc: 0.6334 - val_loss: 1.2031 - val_acc: 0.5724\n",
      "Epoch 15/15\n",
      "50000/50000 [==============================] - 22s 433us/sample - loss: 1.0659 - acc: 0.6318 - val_loss: 1.0603 - val_acc: 0.6343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21cad7b3e10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=15,\n",
    "           validation_data=(x_test, y_test),\n",
    "           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise\n",
    "Our previous model had the structure:\n",
    "\n",
    "Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "(with appropriate activation functions and dropouts)\n",
    "\n",
    "1. Build a more complicated model with the following pattern:\n",
    "- Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "- Use strides of 1 for all convolutional layers.\n",
    "\n",
    "2. How many parameters does your model have?  How does that compare to the previous model?\n",
    "\n",
    "3. Train it for 5 epochs.  What do you notice about the training time, loss and accuracy numbers (on both the training and validation sets)?\n",
    "\n",
    "5. Try different structures and run times, and see how accurate your model can be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "model_2 = Sequential([\n",
    "    Conv2D(32, kernel_size=(3,3), padding='same', input_shape=x_train.shape[1:], activation='relu'),\n",
    "    Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    \n",
    "    \n",
    "    Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'),\n",
    "    Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    \n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Check number of parameters\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While model_1 has 181,162 params, model_2 has 1,250,858 params, which is roughly 1.1 million more params to train. Due to the massive increase in params to train this model takes longer to train even if we iterate through the dataset fewer times (e.g., lower epoch #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "19424/50000 [==========>...................] - ETA: 1:26 - loss: 1.7879 - acc: 0.3400"
     ]
    }
   ],
   "source": [
    "# initiate RMSprop optimizer\n",
    "batch_size = 32\n",
    "\n",
    "opt_2 = keras.optimizers.RMSprop(lr=.0005)\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "               optimizer=opt_2,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_2.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=5,\n",
    "           validation_data=(x_test, y_test),\n",
    "           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model_2's training time is longer taking a little over 12 minutes, taking 7 minutes longer than model_1 to train.\n",
    "\n",
    "The loss values followed a downward trend for both the training and testing/validation sets.  Both sets' accuracy values also increase at each iteration.  The final loss and accuracy score of model_2 is also better than model_1's scores.  One surprising thing to note is the loss value and accuracy scores for the validation set is better than the training set's results for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential([\n",
    "    Conv2D(256, kernel_size=(3,3), padding='same', input_shape=x_train.shape[1:], activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    \n",
    "    Conv2D(128, kernel_size=(3,3), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(64, kernel_size=(3,3), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "opt_3 = keras.optimizers.RMSprop(lr=.01)\n",
    "\n",
    "model_3.compile(loss='categorical_crossentropy',\n",
    "               optimizer=opt_3,\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_3.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=25,\n",
    "           validation_data=(x_test, y_test),\n",
    "           shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
