{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Review and Gradient Descent Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook, we will solve a simple linear regression problem by gradient descent.  \n",
    "We will see the effect of the learning rate on the trajectory in parameter space.\n",
    "We will show how Stochastic Gradient Descent (SGD) differs from the standard version, and the effect of \"shuffling\" your data during SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preliminaries - packages to load\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data from a known distribution\n",
    "Below we will generate data a known distribution.  \n",
    "Specifically, the true model is:\n",
    "\n",
    "$Y = b + \\theta_1 X_1 + \\theta_2 X_2 + \\epsilon$\n",
    "\n",
    "$X_1$ and $X_2$ have a uniform distribution on the interval $[0,10]$, while `const` is a vector of ones (representing the intercept term).\n",
    "\n",
    "We set actual values for $b$ ,$\\theta_1$, and $\\theta_2$\n",
    "\n",
    "Here $b=1.5$, $\\theta_1=2$, and $\\theta_2=5$\n",
    "\n",
    "We then generate a vector of $y$-values according to the model and put the predictors together in a \"feature matrix\" `x_mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X vector\n",
    "# create weights vector\n",
    "# create y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the \"Right\" answer directly\n",
    "In the below cells we solve for the optimal set of coefficients.  Note that even though the true model is given by:\n",
    "\n",
    "$b=1.5$, $\\theta_1=2$, and $\\theta_2=5$\n",
    "\n",
    "The maximum likelihood (least-squares) estimate from a finite data set may be slightly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise:\n",
    "Solve the problem two ways: \n",
    "1. By using the scikit-learn LinearRegression model\n",
    "2. Using matrix algebra directly via the formula $\\theta = (X^T X)^{-1}X^Ty$\n",
    "\n",
    "Note: The scikit-learn solver may give a warning message, this can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create linear regression model\n",
    "# fit model\n",
    "# display learned weights and error coefficent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy vectorization math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving by Gradient Descent\n",
    "\n",
    "\n",
    "For most numerical problems, we don't / can't know the underlying analytical solution. This is because we only arrive at analytical solutions by solving the equations mathematically, with pen and paper. That is more often than not just impossible. Fortunately, we have a way of converging to an approximate solution, by using **Gradient Descent**.\n",
    "\n",
    "\n",
    "We will explore this very useful method because Neural Networks, along with many other complicated algorithms, are trained using Gradient Descent.  Seeing how gradient descent works on a simple example will build intuition and help us understand some of the nuances around setting the learning rate and other parameters.  We will also explore Stochastic Gradient Descent and compare its behavior to the standard approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The next several cells have code to perform (full-batch) gradient descent.  We have omitted some parameters for you to fill in.\n",
    "\n",
    "1. Pick a learning rate, and a number of iterations, run the code, and then plot the trajectory of your gradient descent.\n",
    "1. Find examples where the learning rate is too high, too low, and \"just right\".\n",
    "1. Look at plots of loss function under these conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid search the learning rate and epoch hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain model with to high (.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrian model with to low (.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "Rather than average the gradients across the whole dataset before taking a step, we will now take a step for every datapoint.  Each step will be somewhat of an \"overreaction\" but they should average out.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "The below code runs Stochastic Gradient descent, but runs through the data in the same order every time.  \n",
    "\n",
    "1. Run the code and plot the graphs.  What do you notice?\n",
    "2. Modify the code so that it randomly re-orders the data.  How do the sample trajectories compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the input data\n",
    "# use SGD to fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the parameters below and observe the trajectory it results in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
