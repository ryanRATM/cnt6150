{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skipgrams in Keras\n",
    "\n",
    "- In this lecture, we will implement Skipgrams in `Keras`.\n",
    "\n",
    "#### Loading in and preprocessing data\n",
    "- Load the Alice in Wonderland data in Corpus using Keras utility\n",
    "- `Keras` has some nice text preprocessing features too!\n",
    "- Split the text into sentences.\n",
    "- Use `Keras`' `Tokenizer` to tokenize sentences into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import SVG\n",
    "%matplotlib inline\n",
    "\n",
    "# nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# keras\n",
    "np.random.seed(13)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text\n",
    "from keras.utils.vis_utils import model_to_dot \n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We'll use Alice in Wonderland\n",
    "path = get_file('carrol-alice.txt', origin='http://www.gutenberg.org/files/11/11-0.txt')\n",
    "corpus = open(path, encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1104 1104\n"
     ]
    }
   ],
   "source": [
    "# Split document into sentences first\n",
    "corpus = corpus[corpus.index('\\n\\n')+2:]\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "base_filter = '!\"#$%&()*+,-./:;`<=>?@[\\\\]^_{|}~\\t\\n' + \"'\"\n",
    "tokenizer = Tokenizer(filters=base_filter)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "print(len(sequences), tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Keep your temper,” said the Caterpillar.\n",
      "[2354, 66, 769, 2, 9, 1, 166]\n"
     ]
    }
   ],
   "source": [
    "# To understand what is happening;\n",
    "print(sentences[324])\n",
    "print(sequences[324])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Skipgrams: Generating Input and Output Labels\n",
    "- Now that we have sentences, and word tokenization, we are in good position to create our training set for skipgrams.\n",
    "- Now we need to generate our `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first see how Keras' skipgrams function works.\n",
    "couples, labels = skipgrams(sequences[324],\n",
    "                            len(tokenizer.word_index) + 1,\n",
    "                            window_size=2, \n",
    "                            negative_samples=0, \n",
    "                            shuffle=True, \n",
    "                            categorical=False, \n",
    "                            sampling_table=None)\n",
    "\n",
    "index_2_word = {val : key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "for w1, w2 in couples:\n",
    "    if w1 == 13:\n",
    "        print(index_2_word[w1], index_2_word[w2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate the inputs and outputs for all windows\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "dim = 100\n",
    "window_size = 2\n",
    "\n",
    "def generate_data(sequences, window_size, vocab_size):\n",
    "    for seq in sequences:\n",
    "        X, y = [], []\n",
    "        couples, _ = skipgrams(seq, \n",
    "                              vocab_size, \n",
    "                              window_size=window_size, \n",
    "                              negative_samples=0, \n",
    "                              shuffle=True, \n",
    "                              categorical=False, \n",
    "                              sampling_table=None)\n",
    "        \n",
    "        if not couples:\n",
    "            continue\n",
    "        \n",
    "        for in_word, out_word in couples:\n",
    "            X.append(in_word)\n",
    "            y.append(np_utils.to_categorical(out_word, vocab_size))\n",
    "            \n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = X.reshape(len(X), 1)\n",
    "        y = y.reshape(len(X), vocab_size)\n",
    "        yield X, y\n",
    "        \n",
    "data_generator = generate_data(sequences, window_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Creating the Model\n",
    "- Lastly, we create the (shallow) network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Keras model and view it \n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=vocab_size, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim,)))\n",
    "skipgram.add(Dense(input_dim=dim, units=vocab_size, activation='softmax'))\n",
    "#SVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Compiling and Training\n",
    "- Time to compile and train\n",
    "- We use crossentropy, common loss for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss is 8942.210822105408\n",
      "iteration 1, loss is 8941.840028762817\n",
      "iteration 2, loss is 8941.469831466675\n",
      "iteration 3, loss is 8941.099581718445\n",
      "iteration 4, loss is 8940.72947883606\n",
      "iteration 5, loss is 8940.359350204468\n",
      "iteration 6, loss is 8939.989276885986\n",
      "iteration 7, loss is 8939.618958473206\n",
      "iteration 8, loss is 8939.248700141907\n",
      "iteration 9, loss is 8938.878479003906\n"
     ]
    }
   ],
   "source": [
    "# Compile the Keras Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "sgd = SGD(lr=1e-4, decay=1e-6, momentum=0.9)\n",
    "\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "for iteration in range(10):\n",
    "    loss = 0\n",
    "    for x, y in generate_data(sequences, window_size, vocab_size):\n",
    "        loss = loss + skipgram.train_on_batch(x, y)\n",
    "        \n",
    "    print('iteration {}, loss is {}'.format(iteration, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams: Looking at the vectors\n",
    "\n",
    "To get word_vectors now, we look at the weights of the first layer.\n",
    "\n",
    "Let's also write functions giving us similarity of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0213647037744522\n",
      "\n",
      "shakespeare    1.322887\n",
      "they’d         1.313449\n",
      "shelves        1.306200\n",
      "came           1.293437\n",
      "contents       1.291765\n",
      "alteration     1.286259\n",
      "game           1.285576\n",
      "are            1.280995\n",
      "sat            1.280045\n",
      "hung           1.269913\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "word_vectors = skipgram.get_weights()[0]\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def get_dist(w1, w2):\n",
    "    i1, i2 = tokenizer.word_index[w1], tokenizer.word_index[w2]\n",
    "    v1, v2 = word_vectors[i1], word_vectors[i2]\n",
    "    return cosine(v1, v2)\n",
    "\n",
    "def get_similarity(w1, w2):\n",
    "    return get_dist(w1, w2)\n",
    "\n",
    "def get_most_similar(w1, n=10):\n",
    "    sims = {word : get_similarity(w1, word) for word in tokenizer.word_index.keys() if word != w1}\n",
    "    sims = pd.Series(sims)\n",
    "    sims.sort_values(inplace=True, ascending=False)\n",
    "    return sims.iloc[:n]\n",
    "\n",
    "\n",
    "print(get_similarity('king', 'queen'))\n",
    "print('')\n",
    "print(get_most_similar('queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Your turn -- Modify the code above to create a CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus[corpus.index('\\n\\n')+2:]\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "base_filter = '!\"#$%&()*+,-./:;`<=>?@[\\\\]^_{|}~\\t\\n' + \"'\"\n",
    "\n",
    "tokenizer_cbow = Tokenizer(filters=base_filter)\n",
    "\n",
    "tokenizer_cbow.fit_on_texts(sentences)\n",
    "sequences = tokenizer_cbow.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object generate_data_cbow at 0x000002A6288D01A8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_cbow = len(tokenizer.word_index)\n",
    "embed_size_cbow = 100\n",
    "window_size_cbow = 2\n",
    "\n",
    "def generate_data_cbow(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        \n",
    "        for ix, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = []\n",
    "            \n",
    "            # set max and min range to say the number of words \n",
    "            # to get around (before/after) the target word\n",
    "            start_ix = ix - window_size\n",
    "            end_ix = ix + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                  for i in range(start_ix, end_ix) \n",
    "                                  if (0 <= i < sentence_length) and (i != ix)\n",
    "                                 ])\n",
    "            \n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield x, y\n",
    "        \n",
    "generate_data_cbow(sequences, window_size_cbow, vocab_size_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['the', 'never', 'herself', 'remarkable'] -> Target (Y): limitation\n",
      "Context (X): ['never', 'limitation', 'remarkable', 'on'] -> Target (Y): herself\n",
      "Context (X): ['limitation', 'herself', 'on', 'like'] -> Target (Y): remarkable\n",
      "Context (X): ['herself', 'remarkable', 'like', 'a'] -> Target (Y): on\n",
      "Context (X): ['remarkable', 'on', 'a', 'federal'] -> Target (Y): like\n",
      "Context (X): ['on', 'like', 'federal', 'for'] -> Target (Y): a\n",
      "Context (X): ['like', 'a', 'for', 'much'] -> Target (Y): federal\n",
      "Context (X): ['a', 'federal', 'much', 'turtle'] -> Target (Y): for\n",
      "Context (X): ['federal', 'for', 'turtle', 'and'] -> Target (Y): much\n",
      "Context (X): ['for', 'much', 'and', 'by'] -> Target (Y): turtle\n",
      "Context (X): ['much', 'turtle', 'by', 'nor'] -> Target (Y): and\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in generate_data_cbow(corpus=sequences, window_size=window_size_cbow, vocab_size=vocab_size_cbow):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            354700    \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3547)              358247    \n",
      "=================================================================\n",
      "Total params: 712,947\n",
      "Trainable params: 712,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size_cbow, output_dim=embed_size_cbow, input_length=window_size_cbow*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size_cbow,)))\n",
    "cbow.add(Dense(vocab_size_cbow, activation='softmax'))\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss is 249324.6297082901\n",
      "iteration 1, loss is 249203.98899269104\n",
      "iteration 2, loss is 249083.53575706482\n",
      "iteration 3, loss is 248963.21166706085\n",
      "iteration 4, loss is 248843.08365631104\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "\n",
    "for iteration in range(5):\n",
    "    loss = 0\n",
    "    for x, y in generate_data_cbow(corpus=sequences, window_size=window_size_cbow, vocab_size=vocab_size_cbow):\n",
    "        loss = loss + cbow.train_on_batch(x, y)\n",
    "        \n",
    "    print('iteration {}, loss is {}'.format(iteration, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0695239901542664\n",
      "\n",
      "sister’s     1.406187\n",
      "unusually    1.318744\n",
      "clock        1.307960\n",
      "viewing      1.284597\n",
      "flamingo     1.282939\n",
      "or           1.279009\n",
      "dinn         1.277009\n",
      "david        1.276449\n",
      "flustered    1.266452\n",
      "earls        1.263939\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "word_vectors_cbow = cbow.get_weights()[0]\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def get_dist_cbow(w1, w2):\n",
    "    i1, i2 = tokenizer_cbow.word_index[w1], tokenizer_cbow.word_index[w2]\n",
    "    v1, v2 = word_vectors_cbow[i1], word_vectors_cbow[i2]\n",
    "    return cosine(v1, v2)\n",
    "\n",
    "def get_similarity_cbow(w1, w2):\n",
    "    return get_dist_cbow(w1, w2)\n",
    "\n",
    "def get_most_similar_cbow(w1, n=10):\n",
    "    sims = {word : get_similarity_cbow(w1, word) for word in tokenizer_cbow.word_index.keys() if word != w1}\n",
    "    sims = pd.Series(sims)\n",
    "    sims.sort_values(inplace=True, ascending=False)\n",
    "    return sims.iloc[:n]\n",
    "\n",
    "\n",
    "print(get_similarity_cbow('king', 'queen'))\n",
    "print('')\n",
    "print(get_most_similar_cbow('king'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
