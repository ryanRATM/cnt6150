{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying CIFAR-10 with Data Augmentation\n",
    "\n",
    "In this exercise, we revisit CIFAR-10 and the networks we previously built.  We will use real-time data augmentation to try to improve our results.\n",
    "\n",
    "When you are done going through the notebook, experiment with different data augmentation parameters and see if they help (or hurt!) the performance of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 6, we built two models.  One was smaller (181K parameters) while the second was larger (1.25M) parameters.  Below we use the smaller model and train it with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucas\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Conv2D(32, (5, 5), strides=(2, 2), padding='same', input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "model_1.add(Conv2D(32, (5, 5), strides=(2, 2)))\n",
    "model_1.add(Activation('relu'))\n",
    "            \n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 181K parameters, even though this is a \"small\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "opt = keras.optimizers.RMSprop(lr=.0005, decay=1e-6)\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "               optimizer=opt,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the `ImageDataGenerator` that we will use to serve images to our model during the training process.  Currently, it is configured to do some shifting and horizontal flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.4553 - acc: 0.0989Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 2.2788 - acc: 0.1088\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 2.4552 - acc: 0.0989 - val_loss: 2.2984 - val_acc: 0.1088\n",
      "Epoch 2/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3193 - acc: 0.1004Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 189us/sample - loss: 2.3056 - acc: 0.1006\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 2.3192 - acc: 0.1004 - val_loss: 2.3025 - val_acc: 0.1006\n",
      "Epoch 3/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3197 - acc: 0.1008Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 192us/sample - loss: 2.3053 - acc: 0.1018\n",
      "1562/1562 [==============================] - 52s 33ms/step - loss: 2.3197 - acc: 0.1008 - val_loss: 2.3026 - val_acc: 0.1018\n",
      "Epoch 4/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3446 - acc: 0.1026Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 188us/sample - loss: 2.3000 - acc: 0.1017\n",
      "1562/1562 [==============================] - 52s 33ms/step - loss: 2.3446 - acc: 0.1026 - val_loss: 2.3028 - val_acc: 0.1017\n",
      "Epoch 5/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3088 - acc: 0.1028Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 3s 341us/sample - loss: 2.3091 - acc: 0.1011\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 2.3088 - acc: 0.1027 - val_loss: 2.3072 - val_acc: 0.1011\n",
      "Epoch 6/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3247 - acc: 0.1011Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 190us/sample - loss: 2.3015 - acc: 0.1006\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 2.3247 - acc: 0.1011 - val_loss: 2.3020 - val_acc: 0.1006\n",
      "Epoch 7/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3159 - acc: 0.0990Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 2.3028 - acc: 0.1012\n",
      "1562/1562 [==============================] - 55s 36ms/step - loss: 2.3159 - acc: 0.0990 - val_loss: 2.3021 - val_acc: 0.1012\n",
      "Epoch 8/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3158 - acc: 0.1007Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 2.3041 - acc: 0.1004\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 2.3158 - acc: 0.1007 - val_loss: 2.3024 - val_acc: 0.1004\n",
      "Epoch 9/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3157 - acc: 0.1004Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 2.3037 - acc: 0.1004\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 2.3157 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.1004\n",
      "Epoch 10/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3141 - acc: 0.1013Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 2.3045 - acc: 0.1006\n",
      "1562/1562 [==============================] - 52s 33ms/step - loss: 2.3141 - acc: 0.1013 - val_loss: 2.3037 - val_acc: 0.1006\n",
      "Epoch 11/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3280 - acc: 0.0981Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 204us/sample - loss: 2.3064 - acc: 0.1027\n",
      "1562/1562 [==============================] - 52s 33ms/step - loss: 2.3280 - acc: 0.0980 - val_loss: 2.3118 - val_acc: 0.1027\n",
      "Epoch 12/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3049 - acc: 0.1002Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 2.3048 - acc: 0.1006\n",
      "1562/1562 [==============================] - 52s 34ms/step - loss: 2.3049 - acc: 0.1002 - val_loss: 2.3031 - val_acc: 0.1006\n",
      "Epoch 13/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3042 - acc: 0.0985Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 2.3049 - acc: 0.1000\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 2.3042 - acc: 0.0986 - val_loss: 2.3027 - val_acc: 0.1000\n",
      "Epoch 14/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3051 - acc: 0.0993Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 193us/sample - loss: 2.3028 - acc: 0.1000\n",
      "1562/1562 [==============================] - 51s 32ms/step - loss: 2.3051 - acc: 0.0993 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 15/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3186 - acc: 0.0993Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 208us/sample - loss: 2.3035 - acc: 0.1001\n",
      "1562/1562 [==============================] - 50s 32ms/step - loss: 2.3186 - acc: 0.0993 - val_loss: 2.3028 - val_acc: 0.1001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22b18c38ba8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_1.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                      steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                      epochs=15,\n",
    "                      validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the performance compare with the non-augmented training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The case takes longer to run due to the model being trained with more images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "### Your Turn\n",
    "\n",
    "1. Experiment above with different settings of the data augmentation parameters.  Can you make the model do better?  Can you make it do worse?\n",
    "\n",
    "2. As in Exercise 6, Build a more complicated model with the following pattern:\n",
    "    - Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "    - Use strides of 1 for all convolutional layers.\n",
    "\n",
    "3. Use data augmentation to train this model.  Can you get better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "model_2.add(Conv2D(32, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "            \n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Check number of parameters\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "batch_size = 32\n",
    "\n",
    "opt_2 = keras.optimizers.RMSprop(lr=.0005)\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "               optimizer=opt_2,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.7355 - acc: 0.3664Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 6s 570us/sample - loss: 1.2274 - acc: 0.5319\n",
      "1562/1562 [==============================] - 152s 97ms/step - loss: 1.7353 - acc: 0.3665 - val_loss: 1.3110 - val_acc: 0.5319\n",
      "Epoch 2/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.4279 - acc: 0.4856Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 6s 628us/sample - loss: 1.0821 - acc: 0.5879\n",
      "1562/1562 [==============================] - 162s 104ms/step - loss: 1.4278 - acc: 0.4856 - val_loss: 1.1426 - val_acc: 0.5879\n",
      "Epoch 3/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3040 - acc: 0.5361Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 6s 618us/sample - loss: 1.0350 - acc: 0.6196\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 1.3041 - acc: 0.5360 - val_loss: 1.0566 - val_acc: 0.6196\n",
      "Epoch 4/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2326 - acc: 0.5648Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 693us/sample - loss: 0.9246 - acc: 0.6344\n",
      "1562/1562 [==============================] - 181s 116ms/step - loss: 1.2327 - acc: 0.5648 - val_loss: 1.0222 - val_acc: 0.6344\n",
      "Epoch 5/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1876 - acc: 0.5795Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 723us/sample - loss: 1.0216 - acc: 0.6245\n",
      "1562/1562 [==============================] - 187s 120ms/step - loss: 1.1874 - acc: 0.5796 - val_loss: 1.1075 - val_acc: 0.6245\n",
      "Epoch 6/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1614 - acc: 0.5910Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 674us/sample - loss: 1.0251 - acc: 0.6520\n",
      "1562/1562 [==============================] - 182s 117ms/step - loss: 1.1615 - acc: 0.5910 - val_loss: 1.0186 - val_acc: 0.6520\n",
      "Epoch 7/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1403 - acc: 0.6041Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 669us/sample - loss: 1.0443 - acc: 0.6258\n",
      "1562/1562 [==============================] - 182s 116ms/step - loss: 1.1403 - acc: 0.6041 - val_loss: 1.0861 - val_acc: 0.6258\n",
      "Epoch 8/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1345 - acc: 0.6069Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 676us/sample - loss: 0.9989 - acc: 0.6393\n",
      "1562/1562 [==============================] - 180s 116ms/step - loss: 1.1349 - acc: 0.6068 - val_loss: 1.0487 - val_acc: 0.6393\n",
      "Epoch 9/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1412 - acc: 0.6089Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 661us/sample - loss: 1.0684 - acc: 0.6334\n",
      "1562/1562 [==============================] - 179s 114ms/step - loss: 1.1413 - acc: 0.6088 - val_loss: 1.0866 - val_acc: 0.6334\n",
      "Epoch 10/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1617 - acc: 0.6037Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 668us/sample - loss: 0.9980 - acc: 0.6596\n",
      "1562/1562 [==============================] - 181s 116ms/step - loss: 1.1617 - acc: 0.6037 - val_loss: 1.0299 - val_acc: 0.6596\n",
      "Epoch 11/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1698 - acc: 0.6005Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 840us/sample - loss: 1.0593 - acc: 0.6491\n",
      "1562/1562 [==============================] - 195s 125ms/step - loss: 1.1697 - acc: 0.6005 - val_loss: 1.0350 - val_acc: 0.6491\n",
      "Epoch 12/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.1851 - acc: 0.5953Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 699us/sample - loss: 0.9675 - acc: 0.6579\n",
      "1562/1562 [==============================] - 192s 123ms/step - loss: 1.1850 - acc: 0.5954 - val_loss: 0.9884 - val_acc: 0.6579\n",
      "Epoch 13/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2070 - acc: 0.5915Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 664us/sample - loss: 1.2345 - acc: 0.5987\n",
      "1562/1562 [==============================] - 189s 121ms/step - loss: 1.2074 - acc: 0.5914 - val_loss: 1.1692 - val_acc: 0.5987\n",
      "Epoch 14/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2254 - acc: 0.5879Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 692us/sample - loss: 1.0303 - acc: 0.6172\n",
      "1562/1562 [==============================] - 183s 117ms/step - loss: 1.2256 - acc: 0.5879 - val_loss: 1.1150 - val_acc: 0.6172\n",
      "Epoch 15/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2421 - acc: 0.5835Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 7s 683us/sample - loss: 0.9538 - acc: 0.6388\n",
      "1562/1562 [==============================] - 189s 121ms/step - loss: 1.2421 - acc: 0.5835 - val_loss: 1.0964 - val_acc: 0.6388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec526449b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=30, # 30 degrees\n",
    "    width_shift_range=5,\n",
    "    height_shift_range=5,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_2.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                      steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                      epochs=15,\n",
    "                      validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
