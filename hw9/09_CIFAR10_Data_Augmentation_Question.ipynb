{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying CIFAR-10 with Data Augmentation\n",
    "\n",
    "In this exercise, we revisit CIFAR-10 and the networks we previously built.  We will use real-time data augmentation to try to improve our results.\n",
    "\n",
    "When you are done going through the notebook, experiment with different data augmentation parameters and see if they help (or hurt!) the performance of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 6, we built two models.  One was smaller (181K parameters) while the second was larger (1.25M) parameters.  Below we use the smaller model and train it with data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lucas\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Conv2D(32, (5, 5), strides=(2, 2), padding='same', input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "model_1.add(Conv2D(32, (5, 5), strides=(2, 2)))\n",
    "model_1.add(Activation('relu'))\n",
    "            \n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 181K parameters, even though this is a \"small\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "opt = keras.optimizers.RMSprop(lr=.0005, decay=1e-6)\n",
    "\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "               optimizer=opt,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the `ImageDataGenerator` that we will use to serve images to our model during the training process.  Currently, it is configured to do some shifting and horizontal flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.4553 - acc: 0.0989Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 2.2788 - acc: 0.1088\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 2.4552 - acc: 0.0989 - val_loss: 2.2984 - val_acc: 0.1088\n",
      "Epoch 2/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3193 - acc: 0.1004Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 189us/sample - loss: 2.3056 - acc: 0.1006\n",
      "1562/1562 [==============================] - 54s 34ms/step - loss: 2.3192 - acc: 0.1004 - val_loss: 2.3025 - val_acc: 0.1006\n",
      "Epoch 3/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3197 - acc: 0.1008Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 192us/sample - loss: 2.3053 - acc: 0.1018\n",
      "1562/1562 [==============================] - 52s 33ms/step - loss: 2.3197 - acc: 0.1008 - val_loss: 2.3026 - val_acc: 0.1018\n",
      "Epoch 4/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3446 - acc: 0.1026Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 188us/sample - loss: 2.3000 - acc: 0.1017\n",
      "1562/1562 [==============================] - 52s 33ms/step - loss: 2.3446 - acc: 0.1026 - val_loss: 2.3028 - val_acc: 0.1017\n",
      "Epoch 5/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3088 - acc: 0.1028Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 3s 341us/sample - loss: 2.3091 - acc: 0.1011\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 2.3088 - acc: 0.1027 - val_loss: 2.3072 - val_acc: 0.1011\n",
      "Epoch 6/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3247 - acc: 0.1011Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 190us/sample - loss: 2.3015 - acc: 0.1006\n",
      "1562/1562 [==============================] - 58s 37ms/step - loss: 2.3247 - acc: 0.1011 - val_loss: 2.3020 - val_acc: 0.1006\n",
      "Epoch 7/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3159 - acc: 0.0990Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 2.3028 - acc: 0.1012\n",
      "1562/1562 [==============================] - 55s 36ms/step - loss: 2.3159 - acc: 0.0990 - val_loss: 2.3021 - val_acc: 0.1012\n",
      "Epoch 8/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3158 - acc: 0.1007Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 196us/sample - loss: 2.3041 - acc: 0.1004\n",
      "1562/1562 [==============================] - 54s 35ms/step - loss: 2.3158 - acc: 0.1007 - val_loss: 2.3024 - val_acc: 0.1004\n",
      "Epoch 9/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3157 - acc: 0.1004Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 2.3037 - acc: 0.1004\n",
      "1562/1562 [==============================] - 53s 34ms/step - loss: 2.3157 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.1004\n",
      "Epoch 10/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3141 - acc: 0.1013Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 195us/sample - loss: 2.3045 - acc: 0.1006\n",
      "1562/1562 [==============================] - 52s 33ms/step - loss: 2.3141 - acc: 0.1013 - val_loss: 2.3037 - val_acc: 0.1006\n",
      "Epoch 11/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3280 - acc: 0.0981Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 204us/sample - loss: 2.3064 - acc: 0.1027\n",
      "1562/1562 [==============================] - 52s 33ms/step - loss: 2.3280 - acc: 0.0980 - val_loss: 2.3118 - val_acc: 0.1027\n",
      "Epoch 12/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3049 - acc: 0.1002Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 200us/sample - loss: 2.3048 - acc: 0.1006\n",
      "1562/1562 [==============================] - 52s 34ms/step - loss: 2.3049 - acc: 0.1002 - val_loss: 2.3031 - val_acc: 0.1006\n",
      "Epoch 13/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3042 - acc: 0.0985Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 198us/sample - loss: 2.3049 - acc: 0.1000\n",
      "1562/1562 [==============================] - 55s 35ms/step - loss: 2.3042 - acc: 0.0986 - val_loss: 2.3027 - val_acc: 0.1000\n",
      "Epoch 14/15\n",
      "1560/1562 [============================>.] - ETA: 0s - loss: 2.3051 - acc: 0.0993Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 193us/sample - loss: 2.3028 - acc: 0.1000\n",
      "1562/1562 [==============================] - 51s 32ms/step - loss: 2.3051 - acc: 0.0993 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 15/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.3186 - acc: 0.0993Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 2s 208us/sample - loss: 2.3035 - acc: 0.1001\n",
      "1562/1562 [==============================] - 50s 32ms/step - loss: 2.3186 - acc: 0.0993 - val_loss: 2.3028 - val_acc: 0.1001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22b18c38ba8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_1.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                      steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                      epochs=15,\n",
    "                      validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the performance compare with the non-augmented training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "### Your Turn\n",
    "\n",
    "1. Experiment above with different settings of the data augmentation parameters.  Can you make the model do better?  Can you make it do worse?\n",
    "\n",
    "2. As in Exercise 6, Build a more complicated model with the following pattern:\n",
    "    - Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "    - Use strides of 1 for all convolutional layers.\n",
    "\n",
    "3. Use data augmentation to train this model.  Can you get better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "model_2.add(Conv2D(32, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "            \n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Check number of parameters\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "batch_size = 32\n",
    "\n",
    "opt_2 = keras.optimizers.RMSprop(lr=.0005)\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "               optimizer=opt_2,\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 2.0124 - acc: 0.2965Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 787us/sample - loss: 1.4210 - acc: 0.4770\n",
      "1562/1562 [==============================] - 227s 145ms/step - loss: 2.0123 - acc: 0.2965 - val_loss: 1.4587 - val_acc: 0.4770\n",
      "Epoch 2/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.5199 - acc: 0.4596Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 823us/sample - loss: 1.3905 - acc: 0.5380\n",
      "1562/1562 [==============================] - 223s 143ms/step - loss: 1.5202 - acc: 0.4596 - val_loss: 1.2850 - val_acc: 0.5380\n",
      "Epoch 3/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.4123 - acc: 0.5022Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 787us/sample - loss: 1.5262 - acc: 0.5380\n",
      "1562/1562 [==============================] - 221s 141ms/step - loss: 1.4126 - acc: 0.5021 - val_loss: 1.3235 - val_acc: 0.5380\n",
      "Epoch 4/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3515 - acc: 0.5273Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 9s 926us/sample - loss: 1.4088 - acc: 0.5382\n",
      "1562/1562 [==============================] - 261s 167ms/step - loss: 1.3518 - acc: 0.5273 - val_loss: 1.2940 - val_acc: 0.5382\n",
      "Epoch 5/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3291 - acc: 0.5384Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 10s 1ms/sample - loss: 1.3084 - acc: 0.5794\n",
      "1562/1562 [==============================] - 265s 170ms/step - loss: 1.3293 - acc: 0.5384 - val_loss: 1.2081 - val_acc: 0.5794\n",
      "Epoch 6/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3088 - acc: 0.5465Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 812us/sample - loss: 1.2715 - acc: 0.5434\n",
      "1562/1562 [==============================] - 239s 153ms/step - loss: 1.3091 - acc: 0.5465 - val_loss: 1.3341 - val_acc: 0.5434\n",
      "Epoch 7/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2987 - acc: 0.5531Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 11s 1ms/sample - loss: 1.0819 - acc: 0.5957\n",
      "1562/1562 [==============================] - 225s 144ms/step - loss: 1.2987 - acc: 0.5531 - val_loss: 1.1526 - val_acc: 0.5957\n",
      "Epoch 8/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2927 - acc: 0.5538Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 10s 1ms/sample - loss: 1.1901 - acc: 0.5618\n",
      "1562/1562 [==============================] - 291s 186ms/step - loss: 1.2926 - acc: 0.5538 - val_loss: 1.2066 - val_acc: 0.5618\n",
      "Epoch 9/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2953 - acc: 0.5540Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 811us/sample - loss: 1.0589 - acc: 0.6069\n",
      "1562/1562 [==============================] - 259s 166ms/step - loss: 1.2955 - acc: 0.5541 - val_loss: 1.1080 - val_acc: 0.6069\n",
      "Epoch 10/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2861 - acc: 0.5599Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 805us/sample - loss: 1.3113 - acc: 0.5630\n",
      "1562/1562 [==============================] - 229s 147ms/step - loss: 1.2863 - acc: 0.5599 - val_loss: 1.2911 - val_acc: 0.5630\n",
      "Epoch 11/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.2963 - acc: 0.5603Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 10s 990us/sample - loss: 1.2363 - acc: 0.6145\n",
      "1562/1562 [==============================] - 244s 156ms/step - loss: 1.2962 - acc: 0.5603 - val_loss: 1.1227 - val_acc: 0.6145\n",
      "Epoch 12/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3094 - acc: 0.5537Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 827us/sample - loss: 1.2209 - acc: 0.6039\n",
      "1562/1562 [==============================] - 248s 159ms/step - loss: 1.3096 - acc: 0.5536 - val_loss: 1.1575 - val_acc: 0.6039\n",
      "Epoch 13/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3254 - acc: 0.5514Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 10s 1ms/sample - loss: 1.2487 - acc: 0.6151\n",
      "1562/1562 [==============================] - 241s 154ms/step - loss: 1.3255 - acc: 0.5513 - val_loss: 1.1748 - val_acc: 0.6151\n",
      "Epoch 14/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3411 - acc: 0.5470Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 8s 842us/sample - loss: 1.2704 - acc: 0.5719\n",
      "1562/1562 [==============================] - 235s 150ms/step - loss: 1.3415 - acc: 0.5470 - val_loss: 1.2102 - val_acc: 0.5719\n",
      "Epoch 15/15\n",
      "1561/1562 [============================>.] - ETA: 0s - loss: 1.3730 - acc: 0.5367Epoch 1/15\n",
      "10000/1562 [================================================================================================================================================================================================] - 9s 882us/sample - loss: 1.3449 - acc: 0.5817\n",
      "1562/1562 [==============================] - 236s 151ms/step - loss: 1.3731 - acc: 0.5366 - val_loss: 1.2262 - val_acc: 0.5817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22b41859940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model_2.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                      steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                      epochs=15,\n",
    "                      validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
